# Data Pipeline Configuration

etl:
  # Data ingestion schedule (cron format)
  schedule: '*/15 * * * *' # Every 15 minutes

  # Data sources
  sources:
    polymarket_api:
      enabled: true
      rate_limit: 100 # requests per minute

    polygon_subgraph:
      enabled: true
      batch_size: 1000

    polygon_rpc:
      enabled: true
      max_retries: 3

  # Storage configuration
  storage:
    bronze:
      format: parquet
      compression: snappy
      partition_by: date

    silver:
      engine: duckdb
      memory_limit: 4GB

    gold:
      retention_days: 90

# Feature computation
features:
  # Time windows for rolling calculations
  windows:
    - 1h
    - 4h
    - 24h
    - 7d

  # Feature types
  types:
    - volume_metrics
    - price_metrics
    - liquidity_metrics
    - network_metrics
    - temporal_metrics

# Risk aggregation
risk:
  # Scoring method
  method: weighted_average

  # Detector weights
  weights:
    whale_trade: 0.15
    coordinated_actor: 0.20
    timing_anomaly: 0.15
    liquidity_shock: 0.10
    price_impact: 0.10
    repeated_pattern: 0.08
    cross_market: 0.08
    new_account: 0.05
    withdrawal_pattern: 0.05
    network_analysis: 0.04

  # Alert thresholds
  thresholds:
    low: 0.3
    medium: 0.5
    high: 0.7
    critical: 0.9
